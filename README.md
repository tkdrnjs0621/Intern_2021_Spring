# 비접촉 터치 시스템 구축과 테미 로봇에서의 적용

서울대학교 연합전공 인공지능반도체공학 2021학년도 봄학기 학생인턴연구





## 1. 프로젝트 설명

본 프로젝트의 목표는 터치스크린에 손을 직접 대지 않고 떨어져서 조작할 수 있는 시스템을 구축하는 것이다.

서울대학교 전기정보공학부 행정실에서 사용하는 테미 로봇에서 위의 시스템이 구현된 데모 어플리케이션을 제작하고, 해당 로봇에서 구동될 어플리케이션의 개발자들이 참조할 수 있는 형태의 sdk를 만들어 이후 사용자들 또한  것을 목표로 한다.

프로젝트의 골격은 아래와 같다.



1.  사용자는 스크린 앞에서 준비 동작, 클릭 동작 등의 여러 손 모양을 취한다. 
2.  Jetson Board와 연결된 카메라에서 스크린과 사용자의 손 모양이 포함된 장면을 실시간으로 촬영한다.
3.  Jetson Board에서 위 카메라로 촬영된 사진에서 사용자의 손 모양과 검지 끝의 좌표를 인식한다.
4.  Jetson Board의 클라이언트에서 블루투스 통신으로 위 데이터를 실시간으로 테미 로봇에 전송한다.
5.  테미 로봇의 데모 어플리케이션(혹은 sdk를 활용하여 개발한 어플리케이션)에서 서버를 열어 해당 데이터를 수신한다.
6.  수신한 데이터를 화면 비율에 맞게 가공하여 좌표를 다시 얻고 손동작에 맞는 명령을 수행한다.



전체적인 구현 방법은 아래와 같다.

- 손 모양과 좌표는 object detection에서 흔히 사용되는 tiny-yolov3를 활용하여 여러 데이터들을 학습 시켜 인식한다.

- Jetson Board의 데이터를 블루투스로 전송할 때는 리눅스 환경의 블루투스 라이브러리인 bluez를 활용하여 기존의 darknet을 수정하여 이용한다.

- 데모 어플리케이션/sdk는 안드로이드 환경에서 BluetoothSocket을 활용하여 java로 개발한다.



## 2. 사용 방법

1. Temi에 TestApp을, Jetson Board에 darknet을 설치한다.

2. TestApp을 구동한 후 블루투스 설정을 킨다.
3. darknet의 bluetoothdevice.txt를 temi robot에 맞게 수정한 후, 쉘에서 $./my.sh를 실행한다



## 3. 개발 현황

2021.05.26)

현재 Jetson Board 쪽의 인식부와 전송부 개발이 완료되었다. (기기 변경에 따른 MAC address 및 통신 channel 등의 minor한 수정만 필요)

Galaxy S10+ 환경에서 실시간으로 Jetson Board에서 송신한 데이터를 수신하여 TextView의 형태로 출력하는 TestApp을 개발하였으며, Jetson Booard와 연계하여 손가락을 인식하였을 때 정상적으로 동작함을 확인하였다.



## 4. 세부 구현

추후 작성.



## 5. 기타

- 아직 Jetson Board/darknet의 수정된 사항이 업로드되지 않음

- darknet을 수정할 때 호환성을 고려하지 않았으므로 기존에 설계된 demo 외의 등의 작업(ex. train)을 할 경우 정상 작동하지 않을 수 있음

